Scripts
*******

The scripts in this directory are meant as helpers for maintenance.

They normally require additional packages which can be easily
installed with `pip`.

They support storing autocomplete-friendly dicts of term lists. This
way we can offer autocomplete widgets that retrieve data from redis
DBs and that can contain millions of sets.


normalize.py
============

When fed with a packed list of keys/terms, generates another list with
the terms normalized followed by the key and the full term. Terms and
keys should be separated by ``&&``.

Requires `dinsort` package. If you are in an active `virtualenv` you
can install everything with::

  (venv)$ pip install dinsort

You need a file ``identifiers.txt.gz`` with all entries to
normalize. Results are written to ``terms.txt``.

We expect lines in ``identifiers.txt.gz`` to look like this::

   123&&Term 1
   123-4&&Other Term with Annotations
   221&&Last sample line

i.e. some identifier (not neccessarily only numbers), followed by
``&&`` followed by the real entry text. If your input file has a
different name or uses a different separator string, then you can
tweak these settings in the ``normalize.py`` header.

You can run ``normalize.py`` like this::

  (venv)$ python normalize.py

The result file can be used as input for really feeding a redis DB
with ``fill-redis.py``.

It will contain three "columns" of which the inner one holds
normalized versions of terms.

"Normalizing" here means: filtering strings through `dinsort` and
stripping any ``<``, ``>`` chars.


fill-redis.py
=============

Reads a list as generated by ``normalize.py`` and feeds the content to
a redis db.

Requires ``redis`` package. If you are in an active `virtualenv` you
can install everything with::

  $ pip install redis

Of course you will need a running redis DB to make all this work.

You will normally want to run ``normalize.py`` before, to create a
suitable list of terms and their normalizations. We will need a file
``terms.txt`` which we work with.

The autocomplete data will be stored in a redis ZSET named
``gnd-autocomplete``.

After storing the data (with the ``fill-redis.py`` script) you can try
to fetch them via the local redis client::

  $ redis-cli
  127.0.0.1:6397> ZRANGEBYLEX gnd-autocomplete [bb + LIMIT 0 5
  1) ...
  2) ...
  3) ...
  4) ...
  5) ...

The ZRANGEBYLEX command might not be documented in your client
(although being available).

  http://redis.io/commands/zrangebylex

Please note, that it was introduced in redis 2.8.9, so your server
should meet this version requirement.

With the command above we asked for the first five entries that start
with ``"bb"``.


Preparing Plone
===============

To enable the above redis store as a source for a Plone behavior, the
Plone instance must be able to access the redis store at runtime.

That also means, that you need a more recent redis server install than
is currently (Nov 2015) provided by Ubuntu. The server should be
version 3 or greater.

If you want to run tests (also recommended) you have to provide even a
more recent client version of redis. This can be accomplished by
installing a redis store from source in the virtualenv you normally
use for your Plone instance (you _do_ use a virtualenv, don't you?).::

  $ virtualenv py27
  $ cd py27
  $ wget http://download.redis.io/releases/redis-3.0.5.tar.gz
  $ tar xzf redis-3.0.5.tar.gz
  $ cd redis-3.0.5
  $ make

Now, additionally do::

  $ make PREFIX=/path/to/py27 install

That's it. You have to enable the virtualenv before running the tests.

Another option would be to simply get your Redis server from a more
recent repository. See for instance the local ``.travis.yml`` for a
different source.

If the redis server is ready and all (or some terms) stored, the
autocompletion of the PSJGNDTerms field (a behavior) should work out
of the box.
